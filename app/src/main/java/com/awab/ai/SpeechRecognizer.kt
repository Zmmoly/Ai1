package com.awab.ai

import android.content.Context
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.util.Log
import org.tensorflow.lite.Interpreter
import java.io.File
import java.io.FileInputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import kotlin.math.sqrt

class SpeechRecognizer(private val context: Context) {

    private var interpreter: Interpreter? = null
    private var audioRecord: AudioRecord? = null
    private var isRecording = false
    
    private val sampleRate = 16000
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    private val bufferSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat)
    private val inputSize = 16000
    
    interface RecognitionListener {
        fun onTextRecognized(text: String)
        fun onError(error: String)
        fun onRecordingStarted()
        fun onRecordingStopped()
        fun onVolumeChanged(volume: Float)
        fun onModelLoaded(modelName: String)
    }
    
    private var listener: RecognitionListener? = null
    
    fun setListener(listener: RecognitionListener) {
        this.listener = listener
    }
    
    /**
     * Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
     */
    fun isModelLoaded(): Boolean {
        return interpreter != null
    }

    /**
     * ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ (Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‡Ø§ØªÙ)
     * Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ø¢Ù† - Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ®ØªØ§Ø± Ø§Ù„Ù…Ù„Ù
     */
    fun loadModelFromFile(filePath: String): Boolean {
        return try {
            val file = File(filePath)
            if (!file.exists()) {
                Log.e(TAG, "âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: $filePath")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                return false
            }
            
            if (!file.name.endsWith(".tflite")) {
                Log.e(TAG, "âŒ ØµÙŠØºØ© Ø®Ø§Ø·Ø¦Ø©: ${file.name}")
                listener?.onError("Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ØµÙŠØºØ© .tflite")
                return false
            }
            
            Log.d(TAG, "ğŸ“‚ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„: ${file.name} (${file.length()} bytes)")
            
            val modelBuffer = loadModelFromPath(file)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            
            Log.d(TAG, "ğŸ”§ Ø¥Ù†Ø´Ø§Ø¡ Interpreter...")
            interpreter = Interpreter(modelBuffer, options)
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${file.name}")
            listener?.onModelLoaded(file.name)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            Log.e(TAG, "Stack trace:", e)
            listener?.onError("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ${e.message}")
            false
        }
    }
    
    /**
     * ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
     */
    fun loadModelFromAssets(modelFileName: String = "speech_model.tflite"): Boolean {
        return try {
            val modelBuffer = loadModelFromAssetsInternal(modelFileName)
            val options = Interpreter.Options().apply {
                setNumThreads(4)
            }
            interpreter = Interpreter(modelBuffer, options)
            
            Log.d(TAG, "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: $modelFileName")
            listener?.onModelLoaded(modelFileName)
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† assets: ${e.message}")
            listener?.onError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ assets")
            false
        }
    }

    private fun loadModelFromPath(file: File): MappedByteBuffer {
        val inputStream = FileInputStream(file)
        val fileChannel = inputStream.channel
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size())
    }
    
    private fun loadModelFromAssetsInternal(modelFileName: String): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelFileName)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    fun startRecording() {
        if (isRecording) {
            Log.w(TAG, "âš ï¸ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
            return
        }
        
        if (interpreter == null) {
            listener?.onError("ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹")
            return
        }

        try {
            audioRecord = AudioRecord(
                MediaRecorder.AudioSource.MIC,
                sampleRate,
                channelConfig,
                audioFormat,
                bufferSize
            )

            if (audioRecord?.state != AudioRecord.STATE_INITIALIZED) {
                listener?.onError("ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ")
                return
            }

            isRecording = true
            audioRecord?.startRecording()
            listener?.onRecordingStarted()
            
            Log.d(TAG, "ğŸ¤ Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...")

            Thread {
                recordAndRecognize()
            }.start()

        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            listener?.onError("ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            isRecording = false
        }
    }

    fun stopRecording() {
        if (!isRecording) {
            return
        }

        isRecording = false
        
        try {
            audioRecord?.stop()
            audioRecord?.release()
            audioRecord = null
            
            listener?.onRecordingStopped()
            Log.d(TAG, "ğŸ›‘ ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
        }
    }

    private fun recordAndRecognize() {
        val audioBuffer = ShortArray(bufferSize)
        val audioData = mutableListOf<Short>()
        
        // Get required audio length from model
        val inputShape = interpreter?.getInputTensor(0)?.shape() ?: intArrayOf(1, 1, 193)
        // Shape is [batch, sequence, features] - we need the LAST dimension for features
        val requiredSize = inputShape[inputShape.size - 1]
        val minSize = maxOf(requiredSize / 4, 32) // At least 32 samples minimum
        
        Log.d(TAG, "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰: ${minSize} samplesØŒ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ${requiredSize} samples")
        
        var silenceCount = 0
        val silenceThreshold = 0.01f
        val silenceDuration = 10 // ~0.6 seconds of silence to auto-process
        
        try {
            while (isRecording) {
                val readSize = audioRecord?.read(audioBuffer, 0, bufferSize) ?: 0
                
                if (readSize > 0) {
                    val volume = calculateVolume(audioBuffer, readSize)
                    listener?.onVolumeChanged(volume)
                    
                    for (i in 0 until readSize) {
                        audioData.add(audioBuffer[i])
                    }
                    
                    val currentSize = audioData.size
                    Log.d(TAG, "ğŸ“Š ØªÙ… ØªØ³Ø¬ÙŠÙ„: $currentSize/$requiredSize Ø¹ÙŠÙ†Ø©")
                    
                    // Detect silence
                    if (volume < silenceThreshold) {
                        silenceCount++
                    } else {
                        silenceCount = 0
                    }
                    
                    // Three conditions to process:
                    // 1. Reached required size
                    // 2. Have minimum AND detected silence
                    // 3. User stopped recording
                    
                    val hasEnoughAudio = currentSize >= minSize
                    val detectedSilence = silenceCount >= silenceDuration
                    val reachedRequired = currentSize >= requiredSize
                    
                    if (reachedRequired || (hasEnoughAudio && detectedSilence)) {
                        if (reachedRequired) {
                            Log.d(TAG, "ğŸ¯ ÙˆØµÙ„ Ø¥Ù„Ù‰ $requiredSize samples - Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
                        } else {
                            Log.d(TAG, "ğŸ¯ ØªÙ… ÙƒØ´Ù Ø³ÙƒÙˆØª Ø¨Ø¹Ø¯ $currentSize samples - Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
                        }
                        
                        val audioArray = audioData.toShortArray()
                        val text = recognizeSpeech(audioArray)
                        
                        if (text.isNotBlank()) {
                            listener?.onTextRecognized(text)
                            Log.d(TAG, "âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: '$text'")
                        } else {
                            Log.w(TAG, "âš ï¸ Ù†ØªÙŠØ¬Ø© ÙØ§Ø±ØºØ©")
                        }
                        
                        // Clear buffer for next recognition
                        audioData.clear()
                        silenceCount = 0
                        Log.d(TAG, "ğŸ”„ ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ")
                    }
                } else {
                    Log.w(TAG, "âš ï¸ readSize <= 0: $readSize")
                }
            }
            
            // When user stops recording, process remaining audio if enough
            if (audioData.size >= minSize) {
                Log.d(TAG, "ğŸ¯ ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹ ${audioData.size} samples - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Ù‡Ø§Ø¦ÙŠØ©...")
                
                val audioArray = audioData.toShortArray()
                val text = recognizeSpeech(audioArray)
                
                if (text.isNotBlank()) {
                    listener?.onTextRecognized(text)
                    Log.d(TAG, "âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: '$text'")
                }
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: ${e.message}")
            e.printStackTrace()
            listener?.onError("Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
        }
        
        Log.d(TAG, "ğŸ Ø§Ù†ØªÙ‡Øª Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
    }

    private fun calculateVolume(buffer: ShortArray, size: Int): Float {
        var sum = 0.0
        for (i in 0 until size) {
            sum += (buffer[i] * buffer[i]).toDouble()
        }
        val rms = sqrt(sum / size)
        return (rms / Short.MAX_VALUE).toFloat()
    }

    private fun recognizeSpeech(audioData: ShortArray): String {
        try {
            // Get input/output tensor info
            val inputTensor = interpreter?.getInputTensor(0)
            val outputTensor = interpreter?.getOutputTensor(0)
            
            val inputShape = inputTensor?.shape() ?: intArrayOf(1, 128000)
            val outputShape = outputTensor?.shape() ?: intArrayOf(1, 100)
            val outputType = outputTensor?.dataType()
            
            Log.d(TAG, "ğŸ“Š Input shape: ${inputShape.contentToString()}")
            Log.d(TAG, "ğŸ“Š Output shape: ${outputShape.contentToString()}")
            Log.d(TAG, "ğŸ“Š Output type: $outputType")
            
            // Get the correct dimension for audio features
            // Shape is [batch, sequence, features] so we need the LAST dimension
            val requiredSize = inputShape[inputShape.size - 1]
            Log.d(TAG, "ğŸ“Š Required: $requiredSize samples")
            
            // Normalize to [-1.0, 1.0] and pad if needed
            val normalized = FloatArray(requiredSize) { i ->
                if (i < audioData.size) {
                    audioData[i] / 32768.0f
                } else {
                    0.0f // Padding with zeros
                }
            }
            
            Log.d(TAG, "ğŸ”§ Normalized ${audioData.size} samples â†’ $requiredSize")
            if (audioData.size < requiredSize) {
                Log.d(TAG, "   Padded with ${requiredSize - audioData.size} zeros")
            }
            
            // Create input buffer
            val inputBuffer = ByteBuffer.allocateDirect(requiredSize * 4)
            inputBuffer.order(ByteOrder.nativeOrder())
            normalized.forEach { inputBuffer.putFloat(it) }
            inputBuffer.rewind()
            
            // Prepare output buffer (Int32)
            val maxOutputLength = outputShape.getOrElse(1) { 100 }
            val outputBuffer = IntArray(maxOutputLength)
            
            Log.d(TAG, "ğŸš€ Running inference...")
            interpreter?.run(inputBuffer, outputBuffer)
            Log.d(TAG, "âœ… Inference completed")
            
            // Decode CTC output
            return decodeCTCOutput(outputBuffer)
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error in recognition: ${e.message}")
            e.printStackTrace()
            return ""
        }
    }
    
    /**
     * CTC Decoding for model output
     */
    private fun decodeCTCOutput(indices: IntArray): String {
        val vocabulary = loadVocabulary()
        val result = StringBuilder()
        var lastIdx = -1
        
        Log.d(TAG, "ğŸ” CTC Decoding...")
        Log.d(TAG, "ğŸ” Output indices (first 20): ${indices.take(20)}")
        Log.d(TAG, "ğŸ” Vocabulary size: ${vocabulary.size}")
        
        var validCount = 0
        for (idx in indices) {
            // Skip blank token (index 0)
            if (idx == 0) continue
            
            // Skip repeated characters (CTC rule)
            if (idx == lastIdx) continue
            
            // Valid character index
            if (idx > 0 && idx < vocabulary.size) {
                val char = vocabulary[idx]
                result.append(char)
                validCount++
                
                if (validCount <= 15) {
                    Log.d(TAG, "  [$validCount] idx=$idx â†’ '$char'")
                }
            } else if (idx != 0) {
                Log.w(TAG, "  âš ï¸ Invalid index: $idx (vocab size: ${vocabulary.size})")
            }
            
            lastIdx = idx
        }
        
        val decoded = result.toString()
        Log.d(TAG, "âœ… CTC Result: '$decoded' (${validCount} chars)")
        
        return decoded
    }

    private fun loadVocabulary(): List<String> {
        return try {
            val vocabulary = mutableListOf<String>()
            context.assets.open("vocabulary.txt").bufferedReader().useLines { lines ->
                lines.forEach { line ->
                    vocabulary.add(line.trim())
                }
            }
            Log.d(TAG, "ğŸ“š Loaded vocabulary: ${vocabulary.size} characters")
            vocabulary
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Error loading vocabulary: ${e.message}")
            emptyList()
        }
    }

    fun cleanup() {
        stopRecording()
        interpreter?.close()
        interpreter = null
        Log.d(TAG, "ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
    }

    companion object {
        private const val TAG = "SpeechRecognizer"
    }
}
